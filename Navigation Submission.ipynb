{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Duelling Deep Q Network for navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "      \n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingQNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(DuelingQNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "      \n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        \n",
    "        self.fc_a1 = nn.Linear(64,action_size)\n",
    "        \n",
    "        self.fc_v1 = nn.Linear(64,1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        a = self.fc_a1(x)\n",
    "        v = self.fc_a1(x)\n",
    "        \n",
    "        q = v.expand_as(a) + (a - a.mean(1, keepdim=True).expand_as(a))\n",
    "    \n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "# SumTree\n",
    "# a binary tree data structure where the parentâ€™s value is the sum of its children\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros(2 * capacity - 1)\n",
    "        self.data = numpy.zeros(capacity, dtype=object)\n",
    "        self.n_entries = 0\n",
    "\n",
    "    # update to the root node\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # find sample on leaf node\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    # store priority and sample\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    # update priority\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    # get priority and sample\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class PERMemory:  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "    beta = 0.4\n",
    "    beta_increment_per_sampling = 0.001\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "        self.len = 0\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add(p, sample)\n",
    "        self.len += 1\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / n\n",
    "        priorities = []\n",
    "\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            \n",
    "\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max()\n",
    "\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 32         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 1e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, duel,double,per,state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.double = double\n",
    "        self.per = per\n",
    "        \n",
    "        if duel:\n",
    "            # Dueling Network\n",
    "            self.qnetwork_local = DuelingQNetwork(state_size, action_size, seed).to(device)\n",
    "            self.qnetwork_target = DuelingQNetwork(state_size, action_size, seed).to(device)\n",
    "        else:\n",
    "            # Q-Network\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "        \n",
    "        # Replay memory\n",
    "        if per:\n",
    "            self.memory = PERMemory(BUFFER_SIZE)\n",
    "        else:\n",
    "            self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Saves the last timestep to memory. If we are in an update step,\n",
    "        call the learn method to update our network on training data.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        state (array_like): current state\n",
    "        action: action taken\n",
    "        reward: reward for taking action a in state s\n",
    "        next_state: the state we are in after the action\n",
    "        done: whether or not the game finished\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        experience = (state,action,reward,next_state,done)\n",
    "        \n",
    "        state_v = torch.from_numpy(np.vstack([state])).float().to(device)\n",
    "        next_state_v = torch.from_numpy(np.vstack([next_state])).float().to(device)\n",
    "        action_v = torch.from_numpy(np.vstack([action])).long().to(device)\n",
    "                                      \n",
    "        # Save experience in replay memory\n",
    "        if self.double:\n",
    "            \n",
    "            #initial Q values of local model\n",
    "            Q_expected = self.qnetwork_local(state_v).gather(1, action_v)\n",
    "            #get the best action according to the local model\n",
    "            next_state_actions = self.qnetwork_local(next_state_v).max(1)[1]\n",
    "            #get the Q values for the next state given the previous action calculated\n",
    "            next_state_values = self.qnetwork_target(next_state_v).gather(1, next_state_actions.unsqueeze(-1))\n",
    "            \n",
    "            Q_targets = reward+(GAMMA * next_state_values.detach() * (1-done))\n",
    "        \n",
    "        else:\n",
    "            # Get max predicted Q values (for next states) from target model\n",
    "            Q_targets_next = self.qnetwork_target(next_state_v).detach().max(1)[0].unsqueeze(1)\n",
    "            # Compute Q targets for current states \n",
    "            Q_targets = reward + (GAMMA * Q_targets_next * (1 - done))\n",
    "\n",
    "            # Get expected Q values from local model\n",
    "            Q_expected = self.qnetwork_local(state_v).gather(1, action_v)\n",
    "    \n",
    "        \n",
    "        error = abs(Q_expected - Q_targets).data[0]\n",
    "#         error = 0.5\n",
    "        \n",
    "        if self.per:\n",
    "            self.memory.add(error,experience)\n",
    "        else:\n",
    "            self.memory.add(experience[0],experience[1],experience[2],experience[3],experience[4])\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                if self.per:\n",
    "                    experiences, idxs, is_weights = self.memory.sample(BATCH_SIZE)\n",
    "                else:\n",
    "                    experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        if self.per:\n",
    "            mini_batch = np.array(experiences).transpose()\n",
    "\n",
    "            states = torch.from_numpy(np.vstack(mini_batch[0])).float().to(device)\n",
    "            actions = torch.from_numpy(np.vstack(mini_batch[1])).long().to(device)\n",
    "            rewards = torch.from_numpy(np.vstack(mini_batch[2])).float().to(device)\n",
    "            next_states = torch.from_numpy(np.vstack(mini_batch[3])).float().to(device)\n",
    "            dones = torch.from_numpy(np.vstack(mini_batch[4]).astype(np.uint8)).float().to(device)\n",
    "        else:\n",
    "            states, actions, rewards, next_states, dones = experiences\n",
    "                            \n",
    "        if self.double:\n",
    "            \n",
    "            #initial Q values of local model\n",
    "            Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "            #get the best action according to the local model\n",
    "            next_state_actions = self.qnetwork_local(next_states).max(1)[1]\n",
    "            #get the Q values for the next state given the previous action calculated\n",
    "            next_state_values = self.qnetwork_target(next_states).gather(1, next_state_actions.unsqueeze(-1))\n",
    "            \n",
    "            Q_targets = rewards+(gamma * next_state_values.detach() * (1-dones))\n",
    "        \n",
    "        else:\n",
    "            # Get max predicted Q values (for next states) from target model\n",
    "            Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "            # Compute Q targets for current states \n",
    "            Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "            # Get expected Q values from local model\n",
    "            Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        Î¸_target = Ï„*Î¸_local + (1 - Ï„)*Î¸_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.qnetwork_local,\"local.pt\")\n",
    "        torch.save(self.qnetwork_target,\"target.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(env, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, double=False, per=False, duel=False):\n",
    "    \n",
    "    print(\"initialise....\")\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    \n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    agent = Agent(duel,double,per,state_size=37, action_size=4, seed=0)\n",
    "    \n",
    "    print(\"start training\")\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0] \n",
    "        score = 0\n",
    "        while True:\n",
    "            action = agent.act(state, eps)                 # select an action\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            \n",
    "            #update\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            score += reward                                # update the score\n",
    "            state = next_state    \n",
    "            if done:\n",
    "                break \n",
    "                \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score) \n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=19.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialise....\n",
      "start training\n",
      "Episode 100\tAverage Score: 0.89\n",
      "Episode 200\tAverage Score: 4.54\n",
      "Episode 300\tAverage Score: 7.12\n",
      "Episode 400\tAverage Score: 8.53\n",
      "Episode 500\tAverage Score: 9.85\n",
      "Episode 600\tAverage Score: 11.04\n",
      "Episode 700\tAverage Score: 11.66\n",
      "Episode 800\tAverage Score: 11.88\n",
      "Episode 900\tAverage Score: 12.70\n",
      "Episode 1000\tAverage Score: 13.53\n",
      "Episode 1100\tAverage Score: 14.53\n",
      "Episode 1200\tAverage Score: 14.45\n",
      "Episode 1300\tAverage Score: 13.40\n",
      "Episode 1400\tAverage Score: 14.34\n",
      "Episode 1500\tAverage Score: 13.12\n",
      "Episode 1600\tAverage Score: 14.43\n",
      "Episode 1700\tAverage Score: 13.52\n",
      "Episode 1800\tAverage Score: 13.56\n",
      "Episode 1900\tAverage Score: 13.40\n",
      "Episode 2000\tAverage Score: 14.51\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "scores = dqn(env, double=False, per=True, duel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
