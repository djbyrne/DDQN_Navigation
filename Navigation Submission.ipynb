{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Duelling Deep Q Network for navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "      \n",
    "        self.fc1 = nn.Linear(state_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingQNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(DuelingQNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "      \n",
    "        self.fc1 = nn.Linear(state_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 32)\n",
    "        \n",
    "        self.fc_a1 = nn.Linear(32,action_size)\n",
    "        \n",
    "        self.fc_v1 = nn.Linear(32,1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        a = self.fc_a1(x)\n",
    "        v = self.fc_a1(x)\n",
    "        \n",
    "        q = v.expand_as(a) + (a - a.mean(1, keepdim=True).expand_as(a))\n",
    "    \n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "# SumTree\n",
    "# a binary tree data structure where the parent’s value is the sum of its children\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros(2 * capacity - 1)\n",
    "        self.data = numpy.zeros(capacity, dtype=object)\n",
    "        self.n_entries = 0\n",
    "\n",
    "    # update to the root node\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # find sample on leaf node\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    # store priority and sample\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    # update priority\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    # get priority and sample\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class PERMemory:  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "    beta = 0.4\n",
    "    beta_increment_per_sampling = 0.001\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "        self.len = 0\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add(p, sample)\n",
    "        self.len += 1\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / n\n",
    "        priorities = []\n",
    "\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            \n",
    "\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max()\n",
    "\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, duel,double,per,state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.double = double\n",
    "        self.per = per\n",
    "        \n",
    "        if duel:\n",
    "            # Dueling Network\n",
    "            self.qnetwork_local = DuelingQNetwork(state_size, action_size, seed).to(device)\n",
    "            self.qnetwork_target = DuelingQNetwork(state_size, action_size, seed).to(device)\n",
    "        else:\n",
    "            # Q-Network\n",
    "            self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "            self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "        \n",
    "        # Replay memory\n",
    "        if per:\n",
    "            self.memory = PERMemory(BUFFER_SIZE)\n",
    "        else:\n",
    "            self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Saves the last timestep to memory. If we are in an update step,\n",
    "        call the learn method to update our network on training data.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        state (array_like): current state\n",
    "        action: action taken\n",
    "        reward: reward for taking action a in state s\n",
    "        next_state: the state we are in after the action\n",
    "        done: whether or not the game finished\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        experience = (state,action,reward,next_state,done)\n",
    "        \n",
    "        state_v = torch.from_numpy(np.vstack([state])).float().to(device)\n",
    "        next_state_v = torch.from_numpy(np.vstack([next_state])).float().to(device)\n",
    "        action_v = torch.from_numpy(np.vstack([action])).long().to(device)\n",
    "                                      \n",
    "        # Save experience in replay memory\n",
    "        if self.double:\n",
    "            \n",
    "            #initial Q values of local model\n",
    "            Q_expected = self.qnetwork_local(state_v).gather(1, action_v)\n",
    "            #get the best action according to the local model\n",
    "            next_state_actions = self.qnetwork_local(next_state_v).max(1)[1]\n",
    "            #get the Q values for the next state given the previous action calculated\n",
    "            next_state_values = self.qnetwork_target(next_state_v).gather(1, next_state_actions.unsqueeze(-1))\n",
    "            \n",
    "            Q_targets = reward+(GAMMA * next_state_values.detach() * (1-done))\n",
    "        \n",
    "        else:\n",
    "            # Get max predicted Q values (for next states) from target model\n",
    "            Q_targets_next = self.qnetwork_target(next_state_v).detach().max(1)[0].unsqueeze(1)\n",
    "            # Compute Q targets for current states \n",
    "            Q_targets = reward + (GAMMA * Q_targets_next * (1 - done))\n",
    "\n",
    "            # Get expected Q values from local model\n",
    "            Q_expected = self.qnetwork_local(state_v).gather(1, action_v)\n",
    "    \n",
    "        \n",
    "        error = abs(Q_expected - Q_targets).data[0]\n",
    "#         error = 0.5\n",
    "        \n",
    "        if self.per:\n",
    "            self.memory.add(error,experience)\n",
    "        else:\n",
    "            self.memory.add(experience)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences, idxs, is_weights = self.memory.sample(BATCH_SIZE)\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "#         states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        mini_batch = np.array(experiences).transpose()\n",
    "            \n",
    "        states = torch.from_numpy(np.vstack(mini_batch[0])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack(mini_batch[1])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack(mini_batch[2])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack(mini_batch[3])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack(mini_batch[4]).astype(np.uint8)).float().to(device)\n",
    "                            \n",
    "        if self.double:\n",
    "            \n",
    "            #initial Q values of local model\n",
    "            Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "            #get the best action according to the local model\n",
    "            next_state_actions = self.qnetwork_local(next_states).max(1)[1]\n",
    "            #get the Q values for the next state given the previous action calculated\n",
    "            next_state_values = self.qnetwork_target(next_states).gather(1, next_state_actions.unsqueeze(-1))\n",
    "            \n",
    "            Q_targets = rewards+(gamma * next_state_values.detach() * (1-dones))\n",
    "        \n",
    "        else:\n",
    "            # Get max predicted Q values (for next states) from target model\n",
    "            Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "            # Compute Q targets for current states \n",
    "            Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "            # Get expected Q values from local model\n",
    "            Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.qnetwork_local,\"local.pt\")\n",
    "        torch.save(self.qnetwork_target,\"target.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(env, n_episodes=1000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, double=False, per=False, duel=False):\n",
    "    \n",
    "    print(\"initialise....\")\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    \n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    agent = Agent(duel,double,per,state_size=37, action_size=4, seed=0)\n",
    "    \n",
    "    print(\"start training\")\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0] \n",
    "        score = 0\n",
    "        while True:\n",
    "            action = agent.act(state, eps)                 # select an action\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            \n",
    "            #update\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            score += reward                                # update the score\n",
    "            state = next_state    \n",
    "            if done:\n",
    "                break \n",
    "                \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score) \n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=19.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialise....\n",
      "start training\n",
      "Episode 88\tAverage Score: 0.093"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donalbyrne/anaconda/envs/drlnd/lib/python3.6/site-packages/ipykernel_launcher.py:44: RuntimeWarning: divide by zero encountered in power\n",
      "/Users/donalbyrne/anaconda/envs/drlnd/lib/python3.6/site-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2841ed6b46c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Banana.app\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-5b3f8fb2adf5>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(env, n_episodes, max_t, eps_start, eps_end, eps_decay, double, per, duel)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m#update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m                                \u001b[0;31m# update the score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-64cd680d7e7c>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-64cd680d7e7c>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/drlnd/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "scores = dqn(env, per=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
